{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbIIZfY2LWTI"
      },
      "source": [
        "## Install Dependency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ze4rh4-VLCvT"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/sithu31296/semantic-segmentation\n",
        "!cd semantic-segmentation && pip install -e .\n",
        "!pip install git+https://github.com/bhbbbbb/stas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKnuZUYULfhP"
      },
      "source": [
        "## Import Essential Package"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gt2BMaIgLa8Q"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import torch\n",
        "\n",
        "from semseg.models.segformer import SegFormer\n",
        "\n",
        "from stas.config import Config as StasConfig\n",
        "from stas.stas_dataset import StasDataset # , get_labels_ratio\n",
        "from stas.stas_model_utils import StasModelUtils"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7Pi9WgBMuuC"
      },
      "source": [
        "## Download Dataset and Pretrained weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V3UL2Wi_M7R_"
      },
      "outputs": [],
      "source": [
        "pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPSTkemwM-Tj"
      },
      "source": [
        "## Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mRyxlXBiMm_D"
      },
      "outputs": [],
      "source": [
        "class Config(StasConfig):\n",
        "    DATASET_ROOT: str = 'dataset' # TODO\n",
        "\n",
        "    IMGS_ROOT: str = os.path.join(DATASET_ROOT, 'Train_Images')\n",
        "    MASK_ROOT: str = os.path.join(DATASET_ROOT, 'Train_Masks')\n",
        "    TRAIN_SPLIT: str = os.path.join(DATASET_ROOT, 'split_train.json')\n",
        "    VALID_SPLIT: str = os.path.join(DATASET_ROOT, 'split_valid.json')\n",
        "\n",
        "    log_dir = 'log'\n",
        "    pretrained = os.path.join('..', 'pretrained_models', 'mit_b0.pth')\n",
        "  \n",
        "config = Config()\n",
        "# check implementation and all the registed checking hooks\n",
        "# and freeze(cannot reassign attr. of config anymore.)\n",
        "config.check_and_freeze()\n",
        "\n",
        "# display configurations to console\n",
        "config.display()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hw2BqUxJNngB"
      },
      "source": [
        "## Train or Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pLKjoQmyN-rY"
      },
      "outputs": [],
      "source": [
        "def inference(config: Config, utils: StasModelUtils):\n",
        "    inf_set = StasDataset(config, 'inference')\n",
        "    utils.splash(inf_set, num_of_output=5, out_dir='splash')\n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CMthP-tUNmeT"
      },
      "outputs": [],
      "source": [
        "train_set = StasDataset(config, 'train')\n",
        "valid_set = StasDataset(config, 'val')\n",
        "\n",
        "model = SegFormer(config.backbone, config.num_classes)\n",
        "\n",
        "# start new training\n",
        "utils = StasModelUtils.start_new_training(model, config)\n",
        "\n",
        "# load from last checkpoint\n",
        "# utils = StasModelUtils.load_last_checkpoint(model, config)\n",
        "\n",
        "# or load from particular checkpoint\n",
        "# path = '/path/to/checkponit'\n",
        "# utils = StasModelUtils.load_checkpoint(model, path, config)\n",
        "\n",
        "epochs = 140\n",
        "utils.train(epochs, train_set, valid_set)\n",
        "\n",
        "# do inference\n",
        "# inference(config, utils)\n",
        "\n",
        "print('\\a') # finish alert"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "stas.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
